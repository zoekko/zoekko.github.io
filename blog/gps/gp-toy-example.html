<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Gaussian Processes: A Toy Example</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
      /* Hide Minimal theme's sidebar and site title/description */
      .sidebar, .site-title, .site-description {
        display: none !important;
      }

      /* Make main content full width for a clean look */
      .wrapper {
        margin-left: 0 !important;
        max-width: 950px !important;
        width: 100% !important;
        padding: 32px 24px 32px 24px;
      }

      /* Main container has no text alignment; all content left aligned except headings/images */
      main {
        margin: 0 auto !important;
      }

      /* Center headings only */
      h1, h2 {
        text-align: center;
        font-family: 'Avenir', 'Raleway', -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Helvetica Neue", Arial, sans-serif;
        font-weight: 400;
      }

      h1 {
        font-size: 2.2em;
      }

      h2 {
        font-size: 1.8em;
      }

      a {
        color: #2594af;
        text-decoration: none;
      }

      body {
        font-family: 'Avenir', 'Raleway', -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Helvetica Neue", Arial, sans-serif;
        background: #ffffff;
        color: #222;
        font-weight: 300;
        font-size: 1.1em;
      }

      /* Responsive tweaks for small screens */
      @media (max-width: 500px) {
        .wrapper { padding: 14px 2px; }
        .navbar a { font-size: 16px; }
        .image-container img, .caption { max-width: 98vw !important;}
        .caption { width: 98vw !important;}
      }

      /* Tweak navigation bar spacing, make it look like tabs */
      .navbar {
        text-align: right;
        padding: 1rem 2rem;
        border-bottom: 1px solid #e0e0e0;
      }

      .navbar a {
        margin: 0 16px;
        font-weight: 400;
        font-size: 18px;
      }

      /* Style for the name to match the body text color */
      .name-title {
        color: #222;
        font-size: 1.2em;
      }

      /* Container for images to handle caption positioning - center images */
      .image-container {
        text-align: center;
        margin: 20px 0;
      }

      .image-container img {
        max-width: 600px;
        width: 100%;
        height: auto;
        display: inline-block;
      }

      /* Style for the image caption below image, right-aligned, but inside .image-container so it doesn't full justify */
      .caption {
        font-size: 0.8em;
        color: #777;
        text-align: right;
        margin-top: 5px;
        width: 600px;
        max-width: 100%;
        margin-left: auto;
        margin-right: auto;
        display: block;
      }

      /* Paragraph, lists, blockquotes -- default left alignment */
      p, blockquote, ul, ol {
        text-align: left;
      }

      b {
        font-weight: 800;
      }
    </style>
</head>
<body>
    <div class="navbar">
      <a href="../../../index.html">Home</a>
      <a href="../../../research.html">Research</a>
      <a href="../../../about.html">About Me</a>
      <a href="../../../blog.html">Blog</a>
      <a href="../../../cv.html">CV</a>
    </div>
    <div class="wrapper">
      <main>
        <h1>Gaussian Processes: A Toy Example</h1>

        <div>
            <p>
                When I started learning about Gaussian Processes (GPs), an endeavor still in progress, I came across various definitions, some of which were intuitive:
            </p>
            <blockquote>
                “A Gaussian process model describes a probability distribution over possible functions that fit a set of points.”
                <br>
                <small><a href="https://arxiv.org/html/2009.10862v5" target="_blank">"(Wang, Jie. An intuitive tutorial to Gaussian process regression.)"</a></small>
            </blockquote>
            <p>
                Others were more technical:
            </p>
            <blockquote>
                “A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.”
                <br>
                <small><a href="https://gaussianprocess.org/gpml/chapters/RW.pdf" target="_blank">Source</a></small>
            </blockquote>
            <p>
                Gaussian Processes are powerful modeling tools. What helped me most was seeing a GP in practice with actual data. In this post, I’ll walk through a simple toy example using the
                <a href="https://tinygp.readthedocs.io/en/stable" target="_blank">tinygp library</a>.
                You can find the notebook I used for all the figures
                <a href="https://github.com/zoekko/GP_toy_example" target="_blank">here</a>.
            </p>
        </div>

        <h2>Motivation: Why use a Gaussian Process?</h2>
        <div>
            <p>Suppose you’ve collected a set of observations—maybe the daily temperature, stock prices, or (in my case) a measure of how stellar surfaces vary over time. For this example, let’s stick with temperature over time.</p>
            <p>Imagine you’ve collected a few discrete observations. Maybe you now want to predict tomorrow’s temperature. Or maybe there’s a day in the past with missing data, and you want to estimate the temperature for that day.</p>
            <div class="image-container">
                <img src="data.png" alt="Observed temperature data" />
                <span class="caption">Example discrete observed temperatures over time.</span>
            </div>
            <p>This is a classic regression problem: given observed data, how can you predict values for new inputs?</p>
        </div>

        <h2>How Gaussian Processes Help</h2>
        <div>
            <p><b>GPs allow you to model relationships</b> between your inputs (like days) and outputs (like temperature) without having to choose a specific functional form. Unlike fitting a straight line or a polynomial to your data (where you pre-choose a formula), GPs infer a distribution over possible functions that could explain your data, capturing both <b>predictions</b> and <b>uncertainty</b>.</p>
            <p>
                <b>Key feature:</b> GPs don’t just output a single prediction. They also provide an uncertainty estimate, so you know how confident you are about each prediction. There’s a big difference between predicting that tomorrow’s temperature will be 70° ± 5°, or 70° ± 50°!
            </p>
        </div>

        <h2>A Toy Example: GPs to Predict Temperature</h2>
        <div>
            <p>
                Now let’s run a GP regression on the fake temperature data. I used the <b>Squared Exponential kernel</b>—which determines how smooth or wiggly the predicted function is.
            </p>
            <div class="image-container">
                <img src="gp_no_error.png" alt="GP prediction without error" />
                <span class="caption">GP fit with no measurement error.</span>
            </div>
            <p>
                The GP prediction fits the data perfectly at observed points, with zero uncertainty (assuming perfect measurements).
                As you move further away from observed points, the uncertainty (shaded region) increases. If observations are made close together, there’s less uncertainty between them.
                <br><br>
                But in real life, measurement error exists, so let’s add in some error bars to the observations and update the fit.
            </p>
            <div class="image-container">
                <img src="gp_with_error.png" alt="GP fit with error bars" />
                <span class="caption">GP fit incorporating observational uncertainties.</span>
            </div>
            <p>
                GP fit incorporating observational errors looks more realistic.
                <br>
                Now, the fitted GP lets us <b>interpolate</b> (estimate between known data) or <b>extrapolate</b> (predict beyond the data range), and crucially, tells us how much we can trust each prediction. For example, the temperature on Day 100 is predicted to be 74.3° ± 5.6°. If you predict far beyond the last observation, uncertainty increases, reflecting our lack of information.
            </p>
            <p>
                And that’s an example of how GPs can be used in regression: <b>Given noisy, sparse data, GPs flexibly fit and quantify your uncertainty everywhere.</b>
            </p>
        </div>

        <h2>How does this all actually work?</h2>
        <div>
            <p>
                Let’s revisit the technical definition:
            </p>
            <blockquote>“A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.”</blockquote>
            <p>
                <b>A random variable</b> is simply a quantity whose outcome is uncertain, like the result of a dice roll. Let’s see how this relates to our GP: We can take a slice through the GP at day 100 and just look at the spread of predicted temperature for that day.
            </p>
            <div class="image-container">
                <img src="data_one_marg.png" alt="Single slice of the GP" />
                <span class="caption">GP prediction for a single day (marginal).</span>
            </div>
            <div class="image-container">
                <img src="one_marg.png" alt="Marginal distribution (bell curve)" />
                <span class="caption">Marginal distribution: bell curve for temperature on one day.</span>
            </div>
            <p>
                Marginal distribution: a familiar 1-D Gaussian (bell curve). This curve is described by two values: a mean (most likely temperature) and standard deviation (uncertainty).
            </p>
            <div class="image-container">
                <img src="two_marg.png" alt="Two marginal distributions" />
                <span class="caption">Two marginal distributions: temperatures on day 100 and day 150.</span>
            </div>
            <p>
                Two marginal distributions: the temperature on day 100 and day 150.
                Now we have a collection of random variables: the temperature for each day. The GP is a collection of these random variables.
            </p>
            <p>
                <b>Now, the second part of the definition:</b><br>
                “…any finite number of which have a joint Gaussian distribution.”<br>
                If we plot the predicted temperatures for day 100 and day 150 as a joint distribution, we get a 2D Gaussian:
            </p>
            <div class="image-container">
                <img src="joint_gaussian.png" alt="Joint Gaussian distribution" />
                <span class="caption">Joint distribution: pair of GP predictions (days 100 and 150).</span>
            </div>
            <p>
                Joint distribution: every pair of GP predictions is jointly Gaussian.
                <br>
                This extends to higher dimensions too: any finite set of random variables in a GP is jointly Gaussian. In other words, a GP is an infinite-dimensional generalization of a Gaussian distribution. Every possible input is a random variable, and sets of those random variables are jointly Gaussian.
            </p>
        </div>

        <h2>Components of a GP</h2>
        <div>
            <p>
                While a single Gaussian is defined by a mean and standard deviation, a GP is defined by a <b>mean function</b> and a <b>covariance matrix</b>.
                <br>
                Mean function: The predicted value for every possible input. (For my weather example, I set this as the average observed temperature.)
                <br>
                Covariance matrix: Each pair of points has a quantified correlation—how much knowing one value helps you predict the other. For example, in weather data, day 3 and day 4 are highly correlated, but day 3 and day 100 are not.
            </p>
        </div>

        <h2>Choosing the right GP kernel</h2>
        <div>
            <p>
                The <b>kernel</b> (covariance function) governs these correlations. It encodes your assumptions about how the data should behave: how correlated points should be, how smooth or wiggly the function might be, and whether there are underlying patterns like seasonality.
            </p>
            <ul>
                <li><b>Squared Exponential (SE):</b> Assumes outputs are similar for nearby inputs; favors smooth, gradual changes.</li>
                <li><b>Matern 3/2:</b> Allows for less smoothness (more roughness) in the fitted curve.</li>
                <li><b>ExpSineSquared:</b> Models repeating, periodic patterns.</li>
                <li><b>SE + ExpSineSquared (Composite):</b> Combines smooth trends with periodic/seasonal patterns.</li>
            </ul>
            <div class="image-container">
                <img src="kernel_choice.png" alt="Comparison of GP kernels" />
                <span class="caption">GP predictions using different kernels.</span>
            </div>
            <p><b>Takeaway:</b> Your kernel choice is crucial—it tells the GP what kind of structure to expect in your data. Choose wisely!</p>
        </div>

        <h2>Kernel Hyperparameters: Amplitude & Length Scale</h2>
        <div>
            <p>
                Once you’ve chosen your kernel, you need to set its <b>hyperparameters</b>:
                <br>
                Amplitude: Controls how much the function can vary vertically. Larger amplitude means the GP expects bigger changes.
                <br>
                Length scale: Controls how quickly the function can change. Small length scales let the GP fit rapid changes; large length scales create smoother functions.
            </p>
            <div class="image-container">
                <img src="hyperparameter_choice.png" alt="GP fit with varied amplitude and length scale"/>
                <span class="caption">GP fits with different amplitudes and length scales.</span>
            </div>
            <p>
                Typically, these hyperparameters are <b>fit to the data</b> to avoid underfitting or overfitting, using techniques like maximizing marginal likelihood or Bayesian model comparison (metrics like Bayesian Evidence).
            </p>
        </div>

        <h2>Takeaway</h2>
        <div>
            <b>Gaussian Processes are flexible, uncertainty-aware tools for regression and more:</b>
            <ul>
                <li>They let you fit functions without needing to specify a fixed formula.</li>
                <li>They always provide meaningful confidence estimates for every prediction.</li>
                <li>Your choice of kernel and hyperparameters encodes your assumptions about the data—which makes GPs both powerful and expressive.</li>
            </ul>
            <p>
                Choose kernels and hyperparameters with care, fit them thoughtfully, and your GP can give robust, honest predictions even in complex, noisy scenarios.
            </p>
        </div>
      </main>
    </div>
</body>
</html>
