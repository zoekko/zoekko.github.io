<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Gaussian Processes: A Toy Example</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
      /* Hide Minimal theme's sidebar and site title/description */
      .sidebar, .site-title, .site-description {
        display: none !important;
      }

      /* Make main content full width for a clean look */
      .wrapper {
      margin: 0 auto !important;         /* <-- THIS CENTERS YOUR CONTENT */
      max-width: 950px !important;
      width: 100% !important;
      padding: 32px 24px 32px 24px;
      box-sizing: border-box;
    }

      /* Main container has no text alignment; all content left aligned except headings/images */
      main {
        margin: 0 auto !important;
      }

      /* Center headings only */
      h1, h2 {
        text-align: center;
        font-family: 'Avenir', 'Raleway', -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Helvetica Neue", Arial, sans-serif;
        font-weight: 400;
      }

      h1 {
        font-size: 2.2em;
      }

      h2 {
        font-size: 1.8em;
      }

      a {
        color: #2594af;
        text-decoration: none;
      }

      body {
        font-family: 'Avenir', 'Raleway', -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Helvetica Neue", Arial, sans-serif;
        background: #ffffff;
        color: #222;
        font-weight: 300;
        font-size: 1.1em;
      }

      /* Responsive tweaks for small screens */
      @media (max-width: 500px) {
        .wrapper { padding: 14px 2px; }
        .navbar a { font-size: 16px; }
        .image-container img, .caption { max-width: 98vw !important;}
        .caption { width: 98vw !important;}
      }

      /* Tweak navigation bar spacing, make it look like tabs */
      .navbar {
        text-align: right;
        padding: 1rem 2rem;
        border-bottom: 1px solid #e0e0e0;
      }

      .navbar a {
        margin: 0 16px;
        font-weight: 400;
        font-size: 18px;
      }

      /* Style for the name to match the body text color */
      .name-title {
        color: #222;
        font-size: 1.2em;
      }

      /* Container for images to handle caption positioning - center images */
      .image-container {
        text-align: center;
        margin: 20px 0;
      }

      .image-container img {
        max-width: 600px;
        width: 100%;
        height: auto;
        display: inline-block;
      }

      /* Style for the image caption below image, right-aligned, but inside .image-container so it doesn't full justify */
      .caption {
        font-size: 0.8em;
        color: #777;
        text-align: right;
        margin-top: 5px;
        width: 600px;
        max-width: 100%;
        margin-left: auto;
        margin-right: auto;
        display: block;
      }

      /* Paragraph, lists, blockquotes -- default left alignment */
      p, blockquote, ul, ol {
        text-align: left;
      }

      b {
        font-weight: 800;
      }
    </style>
</head>
<body>
    <div class="navbar">
      <a href="../../../index.html">Home</a>
      <a href="../../../research.html">Research</a>
      <a href="../../../about.html">About Me</a>
      <a href="../../../blog.html">Blog</a>
      <a href="../../../cv.html">CV</a>
    </div>
    <div class="wrapper">
      <main>
        <h1>Gaussian Processes: A Toy Example</h1>

        <div>
            <p>
                When I started learning about Gaussian Processes (GPs), an endeavor that is still in progress, I came across various definitions, some of which were a bit intuitive:
            </p>
            <blockquote>
                “A Gaussian process model describes a probability distribution over possible functions that fit a set of points.”
                <br>
                <small><a href="https://arxiv.org/html/2009.10862v5" target="_blank">(Wang, Jie. An intuitive tutorial to Gaussian process regression.)</a></small>
            </blockquote>
            <p>
                and others less so:
            </p>
            <blockquote>
                “A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.” (though I realized that less intuitive did not mean less important)
                <br>
                <small><a href="https://gaussianprocess.org/gpml/chapters/RW.pdf" target="_blank">(Williams, Christopher KI, and Carl Edward Rasmussen. Gaussian processes for machine learning.)</a></small>
            </blockquote>
            <p>
                Gaussian Processes are a powerful modeling tool, and what helped me the most was seeing a GP in practice with actual data. In this post, I’ll walk through a simple toy example using the
                <a href="https://tinygp.readthedocs.io/en/stable" target="_blank">tinygp library</a>.
                You can find the notebook I used for all the figures
                <a href="https://github.com/zoekko/GP_toy_example" target="_blank">here</a>.
            </p>
        </div>

        <h2>Motivation: Why use a Gaussian Process?</h2>
        <div>
            <p>Suppose you’ve collected a set of observations—maybe the daily temperature, stock prices, or (in my case) a measure of how stellar surfaces vary over time. For this example, let’s stick with temperature over time.</p>
            <p>Imagine you’ve collected a few discrete observations. Maybe you now want to predict tomorrow’s temperature. Or maybe there’s a day in the past with missing data, and you want to estimate the temperature for that day.</p>
            <div class="image-container">
                <img src="data.png" alt="Observed temperature data" />
                <span class="caption">Example discrete observed temperatures over time.</span>
            </div>
            <p>This is a classic regression problem: given observed data, how can you predict values for new inputs?</p>
        </div>

        <h2>How Gaussian Processes Help</h2>
        <div>
            <p>GPs allow you to model relationships between your inputs (like days) and outputs (like temperature) without having to choose a specific functional form. Unlike fitting a straight line or a polynomial to your data (where you pre-choose a formula), GPs don't require you to make that choice beforehand. GPs are different in that (returning to the first GP definition), “GPs infer a distribution over possible functions that could explain the data, capturing both predictions and uncertainty.”
            <p>
                A key feature of GPs is that they don’t just output a single prediction. They also provide an uncertainty estimate, so you know how confident you are about each prediction. There’s a big difference between predicting that tomorrow’s temperature will be 70° ± 5°, or 70° ± 50°.
            </p>
        </div>

        <h2>A Toy Example: GPs to Predict Temperature</h2>
        <div>
            <p>
                Now let’s run a GP regression on the fake temperature data. I used the Squared Exponential kernel, which I will discuss later, but these kernels essentially determine how smooth or wiggly the predicted function is. Let's take a look at the GP's prediction.
            </p>
            <div class="image-container">
                <img src="gp_no_error.png" alt="GP prediction without error" />
                <span class="caption">GP fit with no measurement error.</span>
            </div>
            <p>
                At the observed points, the GP prediction matches the data with very zero uncertainty. This is assuming we measured the temperature perfectly. As you move further away from the observations, the uncertainty (shaded region) increases. If observations are made very close together, there is less uncertainty between them. But in real life, measurement error exists, so let’s add in some error bars to the observations and update the GP fit.
            </p>
            <div class="image-container">
                <img src="gp_with_error.png" alt="GP fit with error bars" />
                <span class="caption">GP fit incorporating observational uncertainties.</span>
            </div>
            <p>
                That looks a bit more realistic.
                <br><br>
                The fitted GP now lets us interpolate (estimate between known data) or extrapolate (predict beyond the data range), and crucially, it tells us how much we can trust each prediction. For example, the temperature on Day 100 is predicted to be 74.3° ± 5.6°. If you predict far beyond the last observation, uncertainty increases, reflecting our lack of information.
            </p>
            <p>
                And that’s an example of how GPs can be used in regression: <b>Given noisy, sparse data, GPs flexibly fit and quantify uncertainty for any input.</b>
            </p>
        </div>

        <h2>How does this all actually work?</h2>
        <div>
            <p>
                Let’s revisit the second GP definition:
            </p>
            <blockquote>“A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.”</blockquote>
            <p>
                A random variable is simply a quantity whose outcome is uncertain, like the result of a dice roll. Let’s see how this relates to our GP: We can take a slice through the GP at day 100 and just look at the spread of predicted temperature for that day.
            </p>
            <div class="image-container">
                <img src="data_one_marg.png" alt="Single slice of the GP" />
                <span class="caption">GP prediction for a single day (marginal).</span>
            </div>
            <div class="image-container">
                <img src="one_marg.png" alt="Marginal distribution (bell curve)" />
                <span class="caption">Marginal distribution: bell curve for temperature on one day.</span>
            </div>
            <p>
                This is the classic bell curve. It describes our single random variable (the temperature on day 100), and it is described by two values: a mean and standard deviation. The mean tells you the most likely value, and the standard deviation tells you the uncertainty on that value.
            <br><br>
                We can also take two slices—one at day 100 and one at day 150—and now we have two marginal distributions, each defined by its own mean and standard deviation. Now we’re looking at two random variables: the temperature on day 100 and the temperature on day 150.
            </p>
            <div class="image-container">
                <img src="two_marg.png" alt="Two marginal distributions" />
                <span class="caption">Two marginal distributions: temperatures on day 100 and day 150.</span>
            </div>
            <p>
                We can now easily extend this to every single point, and now we have a collection of random variables: the temperature of a given day. The GP is simply “a collection of [these] random variables.”
            </p>
            <p>
                Now onto the second part of the definition:<br>
                “…any finite number of which have a joint Gaussian distribution.”
                <br><br>
                If we plot the predicted temperatures for day 100 and day 150 as a joint distribution, we get a 2D Gaussian:
            </p>
            <div class="image-container">
                <img src="joint_gaussian.png" alt="Joint Gaussian distribution" />
                <span class="caption">Joint distribution: pair of GP predictions (days 100 and 150).</span>
            </div>
            <p>
                <br>
                This extends to higher dimensions too: any finite set of random variables in a GP is jointly Gaussian. In other words, a GP is an infinite-dimensional generalization of a Gaussian distribution. Every possible input is a random variable, and sets of those random variables are jointly Gaussian.
            </p>
        </div>

        <h2>Components of a GP</h2>
        <div>
            <p>
                While a single Gaussian distribution is defined by a mean and standard deviation, a GP is now defined by a mean function and a covariance matrix. Going from a mean value to a mean function is fairly straightforward: rather than a single predicted value for a single input value, the mean function now has a predicted value for every single possible input value. It essentially tells the GP what to fit around. For the weather example, I set the mean to be the average observed temperature. 
                <br><br>
                The covariance matrix is slightly more complicated than just a simple extension of a standard deviation. Not only does each point have its uncertainty, but each pair of points has a quantified correlation—how much knowing one helps predict the other. For instance, in the weather example, the temperature on day 3 is likely to be similar to day 4 (highly correlated), but not necessarily similar to day 100 (weakly correlated). The covariance matrix captures the correlations between data points.
            </p>
        </div>

        <h2>Choosing the right GP kernel</h2>
        <div>
            <p>
                The GP kernel governs these correlations. It encodes your assumptions about how the data should behave: how correlated points should be, how smooth or wiggly the function might be, and whether there are underlying patterns like seasonality. 
                <br><br>
                For the weather example, I used a squared exponential (SE) kernel. This kernel basically says: inputs closer together are more highly correlated (nearby temperatures should be similar), and the changes should happen smoothly (temperatures shouldn’t instantaneously spike or drop). 
                <br><br>
                There are many kernels to choose from, each modeling different behaviors. Here are examples of four different kernels that I used to model the same weather data. 
            </p>
            <ul>
                <li><b>Squared Exponential:</b> Assumes outputs are similar for nearby inputs; favors smooth, gradual changes.</li>
                <li><b>Matern 3/2:</b> Allows for less smoothness (more roughness) in the fitted curve.</li>
                <li><b>Exponential Sine Squared:</b> Models repeating, periodic patterns.</li>
                <li><b>Squared Exponential + Exponential Sine Squared (Composite):</b> Combines smooth trends with periodic/seasonal patterns. It is possible to combine any of the kernels to capture complex structure.</li>
            </ul>
            <div class="image-container">
                <img src="kernel_choice.png" alt="Comparison of GP kernels" />
                <span class="caption">GP predictions using different kernels.</span>
            </div>
            <p>In summary, the kernel choice is crucial, as it tells the GP what kind of structure to expect in the data.</p>
        </div>

        <h2>Kernel Hyperparameters: Amplitude & Length Scale</h2>
        <div>
            <p>
                Once you’ve chosen your kernel, you need to set its hyperparameters:
                <ul>
                    <li><b>Amplitude:</b> Controls how much the function can vary vertically. Larger amplitude means the GP expects bigger changes.</li>
                    <li><b>Length scale:</b> Controls how quickly the function can change. Small length scales let the GP fit rapid changes; large length scales create smoother functions.</li>
                </ul>
                Here, I use the squared exponential kernel in all four panels, but I varying the length scale and amplitude.
            </p>
            <div class="image-container">
                <img src="hyperparameter_choice.png" alt="GP fit with varied amplitude and length scale"/>
                <span class="caption">GP fits (squared exponential kernel) with different amplitudes and length scales.</span>
            </div>
            <p>
                Typically, these hyperparameters are fit to the data to avoid underfitting or overfitting, using techniques like maximizing marginal likelihood or Bayesian model comparison (metrics like Bayesian Evidence).
            </p>
        </div>

        <h2>Conclusion</h2>
        <div>
            Gaussian Processes are powerful tools for modeling and prediction amid noisy, limited, or complex data. They flexibly fit functions without needing a rigid formula and assign uncertainties to every prediction. However, it is crucial to carefully choose kernels and tune hyperparameters, which imparts assumptions about the phenomena being modeled. GPs enable robust predictions while communicating the confidence behind every result—a critical advantage in data-driven decision making.
            <p>
            </p>
        </div>
      </main>
    </div>
</body>
</html>
